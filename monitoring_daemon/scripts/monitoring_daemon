#! /usr/bin/python3 -u

import argparse
import logging
import os
import threading

import psutil

from monitoring_daemon import config
from monitoring_daemon.monitor_thread import launch

__author__ = 'Pace Francesco'

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    parser = argparse.ArgumentParser(description='Monitoring Daemon')
    # parser.add_argument('-p', action='store', dest='pid', default=None,
    #                     help='Process ID')
    # parser.add_argument('-F', action='store', dest='log_folder_name', default="system",
    #                     help='Folder Name for logs')
    # parser.add_argument('-f', action='store', dest='process_filter', default=None,
    #                     help='Filter processes spawned the given process')
    # parser.add_argument('--spark', action='store_true', dest='spark', default=False,
    #                     help='Monitor Spark processes (Master and/or Workers)')
    # parser.add_argument('--hdfs', action='store_true', dest='hdfs', default=False,
    #                     help='Monitor HDFS processes (Namenode and/or Datanode)')
    # parser.add_argument('--swift', action='store_true', dest='swift', default=False,
    #                     help='Monitor Swift system')

    # args = parser.parse_args()
    # pid = args.pid
    # process_filter = args.process_filter

    log_folder_path = os.path.abspath(os.path.expandvars(os.path.expanduser(config.LOG_FOLDER)))

    # nl_sock = None
    event = threading.Event()
    event.set()
    threads = []
    try:
        for process in psutil.process_iter():
            cmdline = process.cmdline()
            if "org.apache.spark.deploy.master.Master" in cmdline:
                folder_path = os.path.join(log_folder_path, "Master")
                logger.info("# Monitoring Resources for Spark Master. Logs will be in {}".format(folder_path))
                threads.append(launch(event_listener=event, folder_path=folder_path))
            if "org.apache.spark.deploy.worker.Worker" in cmdline:
                folder_path = os.path.join(log_folder_path, "Worker")
                logger.info("# Monitoring Resources for Spark Worker. Logs will be in {}".format(folder_path))
                threads.append(launch(event_listener=event, folder_path=folder_path))
            if "org.apache.hadoop.hdfs.server.namenode.NameNode" in cmdline:
                folder_path = os.path.join(log_folder_path, "NameNode")
                logger.info("# Monitoring Resources for HDFS NameNode. Logs will be in {}".format(folder_path))
                threads.append(launch(event_listener=event, folder_path=folder_path))
            if "org.apache.hadoop.hdfs.server.datanode.DataNode" in cmdline:
                folder_path = os.path.join(log_folder_path, "DataNode")
                logger.info("# Monitoring Resources for HDFS DataNode. Logs will be in {}".format(folder_path))
                threads.append(launch(event_listener=event, folder_path=folder_path))
        if len(threads) == 0:
            folder_path = os.path.join(log_folder_path, "System")
            logger.info("# Monitoring System Resources. Logs will be in {}".format(folder_path))
            threads.append(launch(event_listener=event, folder_path=folder_path))
    except KeyboardInterrupt:
        logger.info("# KeyboardInterrupt triggered")
        event.clear()
        # if nl_sock is not None:
        #     ret_value = processEvents.unset_proc_ev_listen(nl_sock)
        #     ret_value = processEvents.nl_close(nl_sock)
